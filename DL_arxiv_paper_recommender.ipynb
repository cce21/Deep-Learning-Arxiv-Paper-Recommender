{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/celwer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/celwer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import nltk \n",
    "import string\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Arxiv Research paper data with Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/06 15:30:28 WARN Utils: Your hostname, celwer-XPS-13-7390-2-in-1 resolves to a loopback address: 127.0.1.1; using 192.168.1.29 instead (on interface wlp0s20f3)\n",
      "23/10/06 15:30:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/06 15:30:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('arxiv').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- authors_parsed: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- journal-ref: string (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- report-no: string (nullable = true)\n",
      " |-- submitter: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- update_date: string (nullable = true)\n",
      " |-- versions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- created: string (nullable = true)\n",
      " |    |    |-- version: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\",\"false\") \\\n",
    "      .json(\"./arxiv-metadata-oai-snapshot.json\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_category = ['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG', 'cs.NE', 'cs.RO']\n",
    "filtered_category = ['cs.LG', 'cs.NE']\n",
    "df_filtered=df.filter(df.categories.isin(filtered_category))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15917"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.select('categories').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "json_data = df_filtered.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean abstracts with stemming, lemmatization, removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_abstracts:\n",
    "    all_tokens = []\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for index,row in json_data.iterrows():\n",
    "        sample_string = row['abstract']\n",
    "    #     print(sample_string)\n",
    "        sample_string = re.sub(r'\\d+', '', sample_string)\n",
    "    #     print(sample_string)\n",
    "        sample_string = \"\".join([char.lower() for char in sample_string if char not in string.punctuation])\n",
    "    #     print(sample_string)\n",
    "        tokens = word_tokenize(sample_string)\n",
    "        stem_tokens = []\n",
    "        for token in tokens:\n",
    "            stem_token = porter_stemmer.stem(token)\n",
    "            if stem_token in stopwords.words('english'):\n",
    "                continue\n",
    "            # print(stem_token)\n",
    "            stem_tokens.append(stem_token)\n",
    "        all_tokens.append(stem_tokens)\n",
    "    return all_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tfidf model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_model(all_tokens):\n",
    "    dictionary = corpora.Dictionary(all_tokens)\n",
    "    dictionary.save('/tmp/hep-th.dict')\n",
    "\n",
    "    raw_corpus = [dictionary.doc2bow(token) for token in all_tokens]\n",
    "    corpora.MmCorpus.serialize('/tmp/hep-th.mm', raw_corpus)\n",
    "\n",
    "    dictionary = corpora.Dictionary.load('/tmp/hep-th.dict')\n",
    "    corpus = corpora.MmCorpus('/tmp/hep-th.mm')\n",
    "\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    return(corpus_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### provide top 10 recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given the tfidf model output, provide the top most similar documents to a specific document indicated by doc_ind\n",
    "def most_similar(corpus_tfidf, num_recs, doc_ind):\n",
    "    sim_tfidf = similarities.MatrixSimilarity(corpus_tfidf)\n",
    "    sims = sim_tfidf[corpus_tfidf]\n",
    "    sort_ind = np.argsort(sims[doc_ind])\n",
    "    recs = sort_ind[:-1][:-(num_recs+1):-1]\n",
    "    return(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.00533727, 0.01717856, ..., 0.01486367, 0.02324457,\n",
       "       0.01561839], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_ind = np.argsort(sims[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4648,  5197,  9082, ..., 10149, 12018,     0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 10 most similar\n",
    "num_recs = 10\n",
    "recs = sort_ind[:-1][:-(num_recs+1):-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12018      Self-training (ST), or pseudo-labeling has s...\n",
       "10149      Prediction of protein-ligand (PL) binding af...\n",
       "12454      Partial Label (PL) learning refers to the ta...\n",
       "12556      It is widely believed that given the same la...\n",
       "3280       Gradient Boosted Decision Trees (GBDT) is a ...\n",
       "3842       Convolutional Neural Networks (CNNs) provide...\n",
       "6497       We introduce a method combining variational ...\n",
       "14567      Bayesian Optimization (BO) is typically used...\n",
       "6591       Partial-label learning (PLL) utilizes instan...\n",
       "10585      Semi-supervised learning is a critical tool ...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data.iloc[recs, :]['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def most_similar(df,doc_id,similarity_matrix,matrix):\n",
    "    print (f'Document: {df.iloc[doc_id][\"abstract\"]}')\n",
    "    print ('\\n')\n",
    "    print ('Similar Documents:')\n",
    "    if matrix=='Cosine Similarity':\n",
    "        similar_ix=np.argsort(similarity_matrix[doc_id])[::-1]\n",
    "    elif matrix=='Euclidean Distance':\n",
    "        similar_ix=np.argsort(similarity_matrix[doc_id])\n",
    "    for ix in similar_ix:\n",
    "        if ix==doc_id:\n",
    "            continue\n",
    "        print('\\n')\n",
    "        print (f'Document: {df.iloc[ix][\"abstract\"]}')\n",
    "        print (f'{matrix} : {similarity_matrix[doc_id][ix]}')\n",
    "    return(similar_ix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = df_filtered.limit(20).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Roberta Transformer model, create embeddings and similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('all-distilroberta-v1')\n",
    "\n",
    "document_embeddings = sbert_model.encode(json_data['abstract'])\n",
    "\n",
    "pairwise_similarities=cosine_similarity(document_embeddings)\n",
    "pairwise_differences=euclidean_distances(document_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = most_similar(json_data,0,pairwise_similarities,'Cosine Similarity')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
